{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "## Load, make and save data sets: ##\n",
    "####################################\n",
    "\n",
    "from pipeline.dataops.load import formatted_mnist\n",
    "from pipeline.dataops.preproc import save_gnoisy, save_corrupted\n",
    "\n",
    "# Load MNIST data set from Keras \n",
    "(input_shape, (x_train, y_train),(x_test, y_test)) = formatted_mnist()\n",
    "\n",
    "## Make noisy and corrupted datasets\n",
    "#gauss_params = ((0,8), (0,32),(0,128))\n",
    "#save_gnoisy(x_train, gauss_params)\n",
    "\n",
    "#percent_lerr_tuple = (0.05, 0.15, 0.50)\n",
    "#save_corrupted(percent_lerr_tuple, y_train)\n",
    "\n",
    "# Truncate data (testing only)\n",
    "if True:\n",
    "    x_train = x_train[0:1000,:,:]\n",
    "    y_train = y_train[0:1000]\n",
    "    x_test = x_test[0:500, :, :]\n",
    "    y_test = y_test[0:500]\n",
    "    \n",
    "train, test = (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 1.03619, saving model to ./results/mnist_cnn_scrap/chckpt_weights.00-1.04.hdf5\n",
      "Epoch 00001: val_loss improved from 1.03619 to 0.47838, saving model to ./results/mnist_cnn_scrap/chckpt_weights.01-0.48.hdf5\n",
      "x_train shape: (1000, 28, 28, 1)\n",
      "1000 train samples\n",
      "500 test samples\n",
      "Test loss: 0.478375179291\n",
      "Test accuracy: 0.856000000477\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU. --- Edit this comment and add citation, ma\n",
    "ke sure doc strings are in place.\n",
    "'''\n",
    "from __future__ import print_function # Do we really need this? Where is the old print function called??\n",
    "\"\"\"returns (callbacks, model), where callbacks is a tuple of callbacks\n",
    "\"\"\"\n",
    "import os #avoid importing the entire module\n",
    "import keras #avoid importing the entire module\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "## checkpointer callback imports ##\n",
    "###################################\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from pipeline.keras.callbacks import LossHistory\n",
    "\n",
    "model_id = \"scrap\"\n",
    "\n",
    "x_train = train[0]\n",
    "y_train = train[1]\n",
    "x_test = test[0]\n",
    "y_test = test[1]\n",
    "batch_size = 16\n",
    "num_classes = 10\n",
    "epochs = 2\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# If model directory does not exist, make  one\n",
    "model_name = \"mnist_cnn_{}\".format(model_id)\n",
    "model_dir = \"./results/\" + model_name\n",
    "checkfile = \"/chckpt_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "filepath = model_dir + checkfile\n",
    "if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)\n",
    "\n",
    "## Callbacks\n",
    "loss_history = LossHistory()\n",
    "# Warning: don't use .format() method {} conflicts with keras parsing, tuple out of bounds error\n",
    "checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=0, validation_data=(x_test, y_test), callbacks = [checkpointer, loss_history])\n",
    "# saves model as an HDF5 file\n",
    "model.save(model_dir+\"mnist_cnn_{}\".format(model_name))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882.0\n",
      "Trainable params: 1,199,882.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4783751792907715, 0.85600000047683711]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[loss, accuracy] = model.evaluate(x_test, y_test, verbose=0)\n",
    "error_rate = 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        #TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "################\n",
    "################\n",
    "\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "loaded_model.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the above:\n",
    "\n",
    "#y_pred = model.predict(x_test)\n",
    "#print(class_correct[0]/class_totals[0])\n",
    "#print(\"model accuracy\", accuracy)\n",
    "#print(np.sum(class_correct)/np.sum(class_totals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Notes, Fixes, Etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# In preproc.py\n",
    "#### Image Noise ####\n",
    "## Use hdf5 instead ... faster, more secure, better integration, more scalable??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# shebang line in package files?? Needed??\n",
    "#try to avoid importing the entire module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "@Binil, @biosopher : I've updated the code. Due to Keras switching backends from Theano to Tensorflow, the image dim ordering is altered as well from (channels, width, height) to (width, height, channels). Once this change has been made, the script performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##In confusion matrix, native rounding as local setting to plot function, not working ##\n",
    "#This isn't working\n",
    "#np.set_printoptions(precision=2)\n",
    "\n",
    "# Do we really need this? Where is the old print function called??\n",
    "# tabs or spaces, choose and make uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check MNIST data set is correct\n",
    "# Check that objectives are met\n",
    "# Check style guide and your code: tabs or spaces, etc.\n",
    "# Choose and defend CNN architecture, parameters, optimizers, etc.\n",
    "# Debug the rest of the code in notebook\n",
    "# Finish Answers\n",
    "# Add Filtering image preprocessing, before and after\n",
    "\n",
    "\n",
    "\n",
    "# Add visualizations of convolutional filters on image maps ...\n",
    "# tsne embedding\n",
    "# Add bonus, cross validation of model, end to end done right ... (perhaps without parameter tuning)\n",
    "# Add bonus, train ensemble of models at different label corruptions and plot at a given parameter\n",
    "# Add bonus, implement custom loss function robust against label error corruption, and compare results with previous results.\n",
    "# Time training/cnn diagnostic calculations ... compute memory/resources for each code snippet\n",
    "# plot graph of model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "\n",
    "1. Hypothesis testing on labels, interperet/(if need be) correct results, they don't make sense\n",
    "2. Style guide, coding practices, design, and python idioms\n",
    "2.5 Thoroughly test all code, write unit tests for code in Nose/unittest\n",
    "3. Finish exposition, include paper results/references to back up work\n",
    "3.5 Commit results to new, github repo, and add commits ...\n",
    "4. Cross-Validate, more critically evaluate model ... improve and discuss improvements (easy to discuss/do first)\n",
    "5. Actually extend results, other models reproducing #3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(np.sum(np.multiply(np.squeeze(x_train), np.squeeze(x_train)), axis=1), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[:,:,:,0].shape\n",
    "np.argmax(y_train, axis=1)\n",
    "np.round(8.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
